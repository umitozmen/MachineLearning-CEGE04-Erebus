{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from main import *\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from Load_Ceserian_Dataset import readCeserianFile\n",
    "from keras.utils import to_categorical\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, the online shoppers purchasing intentions dataset will be used to train a neural network. \n",
    "\n",
    "Firstly, we can define the algorithm of a neural network as a mathematical function with flexibility to solve any problem simply by varying the weights. The algorithm is modelled much like the brain and is designed to recognise patterns by containing artificial neurons with the ability to learn. The result of this creation became the well known perceptron which is a model architecture of a binary classifier. However, as the working dataset explored is much more complex, the multilayer perceptron will be used subsequently and includes with the following 3 layers:\n",
    "1. Input - consists of neurons making up the attributes\n",
    "2. Hidden - transforms the layer with a weighted summation including the activation function e.g. Rectified Linear Unit (ReLU)\n",
    "3. Output - receives the values from the last hidden layer and produces the label\n",
    "\n",
    "For example, given one hidden layer and one neuron, the function \n",
    "\n",
    "$$\n",
    "  f(x) = W_{2}g(W^{T}_{1}x+b_{1}+b_{2})\n",
    "$$\n",
    "\n",
    "can be learned by initialising the model parameters \n",
    "$\n",
    "  W_{1}\\in\\R^{m}\n",
    "$\n",
    "and\n",
    "$\n",
    "  \\newcommand{\\R}{\\mathbb{R}}\n",
    "  W_{2},b_{1},b_{2}\\in\\R\n",
    "$\n",
    "With $W_{1}$ and $W_{2}$ as the weights of both the input and hidden layer and finally, $b_{1}$ and $b_{2}$ as the bias within the hidden and output layer. The inputs are then passed through an activation rule which exerts influence on a unit with its current state to produce the output (1). Lastly, the model is tested to see how good it is at predicting based on a given set of parameters by using cross entropy. Model optimisation will also be explored in the sections below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data\n",
    "\n",
    "The data was first examined by importing the csv file which was then later transferred to a pandas dataframe for easy retrieval. As mentioned before in the previous learning tasks, the data is highly categorical and will need to be encoded before it can be used to fit and train the model. The data was then split into features and labels in preparation for the train test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductRelated_Duration</th>\n",
       "      <th>ProductRelatedAve</th>\n",
       "      <th>BounceRates</th>\n",
       "      <th>ExitRates</th>\n",
       "      <th>SpecialDay</th>\n",
       "      <th>Month</th>\n",
       "      <th>Region</th>\n",
       "      <th>VisitorType</th>\n",
       "      <th>Weekend</th>\n",
       "      <th>Revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>627.500000</td>\n",
       "      <td>62.750000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12325</td>\n",
       "      <td>1783.791667</td>\n",
       "      <td>33.656447</td>\n",
       "      <td>0.007143</td>\n",
       "      <td>0.029031</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12326</td>\n",
       "      <td>465.750000</td>\n",
       "      <td>93.150000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12327</td>\n",
       "      <td>184.250000</td>\n",
       "      <td>30.708333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.086667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12328</td>\n",
       "      <td>346.000000</td>\n",
       "      <td>23.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021053</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12329</td>\n",
       "      <td>21.250000</td>\n",
       "      <td>7.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12330 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ProductRelated_Duration  ProductRelatedAve  BounceRates  ExitRates  \\\n",
       "0                     0.000000           0.000000     0.200000   0.200000   \n",
       "1                    64.000000          32.000000     0.000000   0.100000   \n",
       "2                     0.000000           0.000000     0.200000   0.200000   \n",
       "3                     2.666667           1.333333     0.050000   0.140000   \n",
       "4                   627.500000          62.750000     0.020000   0.050000   \n",
       "...                        ...                ...          ...        ...   \n",
       "12325              1783.791667          33.656447     0.007143   0.029031   \n",
       "12326               465.750000          93.150000     0.000000   0.021333   \n",
       "12327               184.250000          30.708333     0.083333   0.086667   \n",
       "12328               346.000000          23.066667     0.000000   0.021053   \n",
       "12329                21.250000           7.083333     0.000000   0.066667   \n",
       "\n",
       "       SpecialDay  Month  Region  VisitorType  Weekend  Revenue  \n",
       "0             0.0      0       1            0        0    False  \n",
       "1             0.0      0       1            0        0    False  \n",
       "2             0.0      0       9            0        0    False  \n",
       "3             0.0      0       2            0        0    False  \n",
       "4             0.0      0       1            0        1    False  \n",
       "...           ...    ...     ...          ...      ...      ...  \n",
       "12325         0.0      9       1            0        1    False  \n",
       "12326         0.0      7       1            0        1    False  \n",
       "12327         0.0      7       1            0        1    False  \n",
       "12328         0.0      7       3            0        0    False  \n",
       "12329         0.0      7       1            1        1    False  \n",
       "\n",
       "[12330 rows x 10 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame_os = read_data_return_frame(\"online_shoppers_intention.csv\")\n",
    "preprocess_df(data_frame_os) # function preprocess_df factorizes the categorical variables\n",
    "data_frame_os # return factorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ProductRelated_Duration  ProductRelatedAve  BounceRates  ExitRates  \\\n",
      "0                     0.000000           0.000000     0.200000   0.200000   \n",
      "1                    64.000000          32.000000     0.000000   0.100000   \n",
      "2                     0.000000           0.000000     0.200000   0.200000   \n",
      "3                     2.666667           1.333333     0.050000   0.140000   \n",
      "4                   627.500000          62.750000     0.020000   0.050000   \n",
      "...                        ...                ...          ...        ...   \n",
      "12325              1783.791667          33.656447     0.007143   0.029031   \n",
      "12326               465.750000          93.150000     0.000000   0.021333   \n",
      "12327               184.250000          30.708333     0.083333   0.086667   \n",
      "12328               346.000000          23.066667     0.000000   0.021053   \n",
      "12329                21.250000           7.083333     0.000000   0.066667   \n",
      "\n",
      "       SpecialDay  Month  Region  VisitorType  Weekend  Revenue  \n",
      "0             0.0      0       1            0        0    False  \n",
      "1             0.0      0       1            0        0    False  \n",
      "2             0.0      0       9            0        0    False  \n",
      "3             0.0      0       2            0        0    False  \n",
      "4             0.0      0       1            0        1    False  \n",
      "...           ...    ...     ...          ...      ...      ...  \n",
      "12325         0.0      9       1            0        1    False  \n",
      "12326         0.0      7       1            0        1    False  \n",
      "12327         0.0      7       1            0        1    False  \n",
      "12328         0.0      7       3            0        0    False  \n",
      "12329         0.0      7       1            1        1    False  \n",
      "\n",
      "[12330 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data_frame_os)\n",
    "features = np.array(data_frame_os.iloc[:,0:9]).astype(np.float)\n",
    "labels =np.array(data_frame_os['Revenue']).astype(np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron Classifier\n",
    "\n",
    "Using Pytorch's library to build the model allowed for the use of the base class of the neural network modules which made development simple. Due to the backward step being fully handled by Pytorch, we can focus on creating the forward step. Firstly the hidden layers needed to be created, in this instance 3 layers were created which will apply the linear transformation to the incoming data. The choice of 3 was purely arbitrary due to the fact that both layers and nodes are model hyperparameters but research from similar datasets suggests that it may be best to go for greater depth as this results in better generalisation for a wider variety of tasks. Furthermore, deep architectures tend to convey useful priors which leads over the function space for the model to learn (2). \n",
    "\n",
    "Switching back to the forward function; this function simply exists to define the network structure that includes the activation function. There are many activation functions availabe such as the commonly used Sigmoid or Tanh but for this model, ReLU was the chosen function and can be defined as: $max(0,z)$. Comparing to the Sigmoid, ReLU is also non-linear given that it takes real values as inputs and provides outputs as 0s and 1s but the only difference is that it generally gives better performance by avoiding vanishing gradient problems (3). Finally the Softmax activation function defined as:\n",
    "$$\n",
    "  \\newcommand{\\euler}{e}\n",
    "  \\sigma(\\vec{z}_{i})=\\frac{e^{z_{i}}}{\\sum_{j=1}^{K}e^{z_{j}}}\n",
    "$$\n",
    "where:\n",
    "\n",
    "$\\vec{z}$ = input vector\n",
    "\n",
    "$e^{z_{i}}$ = standard exponential function for input vector\n",
    "\n",
    "$K$ = number of classes in the multi-class classifier\n",
    "\n",
    "$e^{z_{j}}$ = standard exponential function for output vector\n",
    "\n",
    "was then used for the third layer as it assigns decimal properties to each target class making it the best choice to help speed up the converges of training (4). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Model, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 50)\n",
    "        self.layer2 = nn.Linear(50, 20)\n",
    "        self.layer3 = nn.Linear(20, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.softmax(self.layer3(x))  # To check with the loss function\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Scikit-learn's train test split, training was allocated 75% of the data and remaining to test. As already identified that the dataset exhibits an imbalance in the distribution of target class, the choice to use stratified sampling was the best way forward to preserve the same percentage for each target class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train,features_test, labels_train, labels_test = train_test_split(features, labels, random_state=21, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is to train the model taking performance in to consideration. By defining a loss function that evaluates how well the model does we can then set goals to minimise this lost. As briefly mentioned above, cross entropy was used. In essence, this loss function measures the performance of the model by calculating the sum of the separate loss for each class by the observation (5). This is defined as:\n",
    "$$\n",
    "  -\\sum_{c=1}^{M}y_{o,c}log(p_{o,c})\n",
    "$$\n",
    "where:\n",
    "\n",
    "${M}$ = the number of classes\n",
    "\n",
    "$y$ = represents 0 or 1 if the class label $c$ is the correct classification for observation $o$\n",
    "\n",
    "$p$ = the predicted probability the observatin $o$ is of class $c$\n",
    "\n",
    "\n",
    "Now we can then optimise the model and this was done usig Adaptive Moment Estimation (ADAM) was chosen as opposed to the more well known optimisation method Stochastic Gradient Descent (SGD) is because of their similarity but differs in the matters of adaptive learning. When it comes to parameters that would be given small updates, ADAM gives large updates due to storing both the exponentially decaying average of past squared gradients and the exponentially decarying average of past gradients. In comparison, SGD's learning rate tends to have the same effect for all weights in the model which could slow down and will require manual tuning whereas ADAM converges much faster (6).\n",
    "\n",
    "To use ADAM, the decaying average of past squared gradients and past gradients must be calculated as follows:\n",
    "$$\n",
    "  v_{t}=\\beta_{2}v_{t-1}+(1-\\beta_{2})g_{t}^{2}\n",
    "$$\n",
    "$$\n",
    "  m_{t}=\\beta_{1}m_{t-1}+(1-\\beta_{1})g_{t}\n",
    "$$\n",
    "\n",
    "where $v_{t}$ and $m_{t}$ are the first moment denoting the mean and the second moment denoting the variance of the gradients. However, there is still an issue of a bias towards 0 in the intial first steps so to counter the bias, estimates can be calculated for the first and second moments:\n",
    "$$\n",
    "  \\hat{v}_{t}=\\frac{v_{t}}{1-\\beta_{2}^{t}}\n",
    "$$\n",
    "$$\n",
    "  \\hat{m}_{t}=\\frac{m_{t}}{1-\\beta_{1}^{t}}\n",
    "$$\n",
    "\n",
    "The parameters are then updated which gives the ending ADAM rule:\n",
    "$$\n",
    "  \\theta_{t}=\\theta_{t-1}-\\frac{\\eta}{\\sqrt{\\hat{v_{t}}}+\\epsilon}\\hat{m_{t}}\n",
    "$$\n",
    "\n",
    "On a final note, the epoch value also needs to be stated given the nature of that neural networks is part of an epoch-based alogoritm. An epoch is essentially the number of times the algorithm will see the entire training set not to be confused with iteration which is the number of batches the algorithm has seen. As ADAM is used which is an iterative process, it is not advised to use one epoch. This is due to the fact that one epoch is insufficient in updating weights on a single pass and therefore lead to underfitting (6). In light of this, epoch was set to 100 as it is purely heuristics and gives the model a chance to readjust the parameters so it is not biased towards the last few data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "model = Model(features_train.shape[1])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "After training, the printed results confirms the reduction of the loss function. After that, the function zero_grad was used to clear old gradients from the last step so there is no accumulation of the previous gradients from the backward functions calls. Calling backward() then computes the derivative of the loss with respect to the parameters using backpropagation and finishing with a step call which tells the optimiser to take a step based on the gradients of the parameters. Prediction is then made under F1 score rather than accuracy given then unbalanced nature of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_(loss):\n",
    "    print (\"The loss calculated: \", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 1\n",
      "The loss calculated:  0.9568246603012085\n",
      "Epoch # 2\n",
      "The loss calculated:  0.4685142934322357\n",
      "Epoch # 3\n",
      "The loss calculated:  0.46760451793670654\n",
      "Epoch # 4\n",
      "The loss calculated:  0.46757015585899353\n",
      "Epoch # 5\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 6\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 7\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 8\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 9\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 10\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 11\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 12\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 13\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 14\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 15\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 16\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 17\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\QianChongZi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 19\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 20\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 21\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 22\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 23\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 24\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 25\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 26\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 27\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 28\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 29\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 30\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 31\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 32\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 33\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 34\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 35\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 36\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 37\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 38\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 39\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 40\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 41\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 42\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 43\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 44\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 45\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 46\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 47\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 48\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 49\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 50\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 51\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 52\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 53\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 54\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 55\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 56\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 57\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 58\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 59\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 60\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 61\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 62\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 63\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 64\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 65\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 66\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 67\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 68\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 69\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 70\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 71\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 72\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 73\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 74\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 75\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 76\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 77\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 78\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 79\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 80\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 81\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 82\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 83\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 84\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 85\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 86\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 87\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 88\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 89\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 90\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 91\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 92\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 93\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 94\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 95\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 96\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 97\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 98\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 99\n",
      "The loss calculated:  0.4675697088241577\n",
      "Epoch # 100\n",
      "The loss calculated:  0.4675697088241577\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = Variable(torch.from_numpy(features_train)).float(), Variable(torch.from_numpy(labels_train)).long()\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    print(\"Epoch #\", epoch)\n",
    "    y_pred = model(x_train)\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    print_(loss.item())\n",
    "\n",
    "    # Zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()  # Gradients\n",
    "    optimizer.step()  # Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 0.8439831333117094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\QianChongZi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\QianChongZi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = Variable(torch.from_numpy(features_test)).float()\n",
    "pred = model(x_test)\n",
    "\n",
    "pred = pred.detach().numpy()\n",
    "\n",
    "print(\"The accuracy is\", accuracy_score(labels_test, np.argmax(pred, axis=1)))\n",
    "\n",
    "# Checking for first value\n",
    "np.argmax(model(x_test[0]).detach().numpy(), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography \n",
    "\n",
    "1. 1.17. Neural network models (supervised) â€” scikit-learn 0.24.1 documentation [Internet]. Scikit-learn.org. 2021 [cited 27 March 2021]. Available from: https://scikit-learn.org/stable/modules/neural_networks_supervised.html#mathematical-formulation\n",
    "\n",
    "2. Sze V, Chen Y, Yang T, Emer J. Efficient Processing of Deep Neural Networks: A Tutorial and Survey. Proceedings of the IEEE. 2017;105(12):2295-2329.\n",
    "\n",
    "3. Activation Functions in Neural Networks [Internet]. Medium. 2021 [cited 27 March 2021]. Available from: https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6\n",
    "\n",
    "4. Multi-Class Neural Networks: Softmax  |  Machine Learning Crash Course [Internet]. Google Developers. 2021 [cited 27 March 2021]. Available from: https://developers.google.com/machine-learning/crash-course/multi-class-neural-networks/softmax\n",
    "\n",
    "5. sklearn.metrics.log_loss â€” scikit-learn 0.24.1 documentation [Internet]. Scikit-learn.org. 2021 [cited 27 March 2021]. Available from: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html\n",
    "\n",
    "6. Zhang Z. Improved Adam Optimizer for Deep Neural Networks. 2018 IEEE/ACM 26th International Symposium on Quality of Service (IWQoS). 2018;."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

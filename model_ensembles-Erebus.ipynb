{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Ensembles\n",
    "\n",
    "In this notebook we will implement stacking ensembles models for online shoppers dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Code / Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our code is saved in the same folder with this report under descion_tree.py and data set is also at the same location called \"online_shoppers_intention.csv\". We created a set of definitions to help build our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Desicion_Tree import *\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame_os = read_data_return_frame(\"online_shoppers_intention.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, class_names, feature_names = preprocess_df(data_frame_os)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key idea of stacking is to train multiple classifiers and then stack them together using a meta-learner or a voting mechanism.(Module CEGE0004 Week-06 Lecture Slides)\n",
    "In this case we will use below classifiers:\n",
    "   * Decision tree classifier that we defined earlier \n",
    "   * Naive Bayes Classifiers\n",
    "       * Bernoulli\n",
    "       * Gaussian\n",
    "       * Multinominal\n",
    "   *  KNeighborsClassifier\n",
    "    \n",
    "Let's start with the definition of these classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(x, y, test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desicion Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(criterion='gini')\n",
    "params = {'max_depth': [5,6,7,8,None],\n",
    "          'min_samples_leaf': [5,6,7,8],\n",
    "          'min_samples_split': [5,6,7,8,9]\n",
    "         }\n",
    "grid_search = GridSearchCV(estimator=clf,param_grid=params, scoring='f1')\n",
    "grid_search.fit(x_train, y_train)\n",
    "gcv_desc_tr = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function for different Naive Bayes Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_NB_classifier(x_train, y_train, classifier):\n",
    "       \n",
    "    param_grid = [{'alpha': [0.1,0.5, 1.0, 1.5, 5, 10]}]\n",
    "\n",
    "    if classifier == 'Gaussian':\n",
    "        return GaussianNB()\n",
    "    \n",
    "    elif classifier == \"Multinomial\":\n",
    "        classifier = MultinomialNB()\n",
    "        grid_search = GridSearchCV(classifier, param_grid, cv=5, verbose=2, scoring = 'f1')\n",
    "        grid_search.fit(x_train, y_train)\n",
    "        return grid_search.best_estimator_\n",
    "    \n",
    "    elif classifier == \"Bernoulli\":\n",
    "        classifier = BernoulliNB()        \n",
    "        grid_search = GridSearchCV(classifier, param_grid, cv=5, verbose=2, scoring = 'f1')\n",
    "        grid_search.fit(x_train, y_train)\n",
    "        return grid_search.best_estimator_\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create each NB classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "gcv_multiN = create_NB_classifier(x_train,y_train,'Multinomial')\n",
    "gcv_bern = create_NB_classifier(x_train,y_train,'Bernoulli')\n",
    "clf_gaus = create_NB_classifier(x_train,y_train,'Gaussian');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_Knn = KNeighborsClassifier(n_neighbors=1, metric='cosine', weights = 'uniform')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VotingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the voting mechanism by using the VotingClassifier of scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('dtr', gcv_desc_tr), ('nb_MN', gcv_multiN), ('nb_BE', gcv_bern), ('nb_GA', clf_gaus), ('Knn', clf_Knn)],\n",
    "    voting='hard')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this voting classifier we need to provide an id for each classifier and the classifiers themselves, and a voting parameter which can be set to hard or soft. If this is set to ‘hard’, it uses the predicted class labels for a majority rule voting, if this is set to ‘soft’, it predicts the class label based on the argmax of the sums of the predicted probabilities.\n",
    "\n",
    "We then fit the voting classifier.(Module CEGE0004 Week-06 Lecture Slides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('dtr',\n",
       "                              DecisionTreeClassifier(min_samples_leaf=5,\n",
       "                                                     min_samples_split=9)),\n",
       "                             ('nb_MN', MultinomialNB(alpha=0.1)),\n",
       "                             ('nb_BE', BernoulliNB()), ('nb_GA', GaussianNB()),\n",
       "                             ('Knn',\n",
       "                              KNeighborsClassifier(metric='cosine',\n",
       "                                                   n_neighbors=1))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to fit each classifier independently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf in (gcv_desc_tr, gcv_multiN, gcv_bern, clf_gaus, clf_Knn):\n",
    "    clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier\n",
      "\ttrain: 0.9087271547528928\n",
      "\ttest: 0.7872202400259487\n",
      "MultinomialNB\n",
      "\ttrain: 0.7261814642586785\n",
      "\ttest: 0.7252675964969186\n",
      "BernoulliNB\n",
      "\ttrain: 0.8458959662593274\n",
      "\ttest: 0.840739539409666\n",
      "GaussianNB\n",
      "\ttrain: 0.7621931437222883\n",
      "\ttest: 0.7632176451508271\n",
      "KNeighborsClassifier\n",
      "\ttrain: 0.9998918568184276\n",
      "\ttest: 0.7654881608822576\n",
      "VotingClassifier\n",
      "\ttrain: 0.9049421433978587\n",
      "\ttest: 0.8297113201427181\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (gcv_desc_tr, gcv_multiN, gcv_bern, clf_gaus, clf_Knn, voting_clf):\n",
    "    print(clf.__class__.__name__)\n",
    "    y_pred = clf.predict(x_train)\n",
    "    print('\\ttrain:', accuracy_score(y_train, y_pred))\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print('\\ttest:', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking Classifier of Voting Classifier reports 83% on Test data which is higher than every other type of classifiers except Bernoulli which is 84%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "Lipani, A. (2021) UCL (University College London)Module CEGE0004 Week-05 Practical Material"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
